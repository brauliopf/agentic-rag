from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import ServerlessSpec
from core.config import (
    PINECONE_API_KEY, 
    PINECONE_HOST_URL, 
    PINECONE_INDEX_NAME,
    OPENAI_API_KEY,
    EMBEDDING_MODEL,
    EMBEDDING_DIMENSION
)

pc = Pinecone(api_key=PINECONE_API_KEY)

def init_pinecone_index():
    """Initialize Pinecone client and create index if it doesn't exist. Returns the retrieved index."""
    index_name = PINECONE_INDEX_NAME

    # use create_index to store vectors generated by a third-party embedding model
    if not pc.has_index(index_name):
        pc.create_index(
            name=index_name,
            vector_type="dense",
            dimension=EMBEDDING_DIMENSION,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )

    return pc.Index(index_name)

def get_embeddings_model():
    """Get the OpenAI embeddings model."""
    return OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        openai_api_key=OPENAI_API_KEY
    )

def get_vector_store():
    """Initialize and return a Pinecone vector store with OpenAI embeddings."""
    # Initialize Pinecone
    init_pinecone_index()
    
    # Create embeddings model
    embeddings = get_embeddings_model()
    
    # Create vector store
    vectorstore = PineconeVectorStore(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings,
        text_key="text"  # The key that contains the document text in the metadata
    )
    
    return vectorstore